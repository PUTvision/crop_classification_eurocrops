{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "sha = repo.head.object.hexsha\n",
    "sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "seed = 34\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "location = os.path.abspath('./cachedir')\n",
    "print(f'cache location: {location}')\n",
    "memory = Memory(location, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_PATH = '/media/data/local/eurocrops/m1615987/'\n",
    "H5_FILE_PATH = os.path.join(ROOT_DATA_PATH, 'HDF5s/train/AT_T33UWP_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CHANNELS = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data_from_h5_file(h5_file_path):\n",
    "    hdf = pd.HDFStore(h5_file_path, mode='r') #here we directly load the .h5 file in one go using pandas.\n",
    "    region_names = hdf.keys()  #list all the keys or regions in the region (for eg- AT112)\n",
    "    region_names = region_names #[:3] - potentially limit amount of data\n",
    "    df_datas = []\n",
    "\n",
    "    for region_name in tqdm(region_names):\n",
    "        df_data_single = hdf.get(f'/{region_name}') #selecting a region from based on the key (AT112 for eg.)\n",
    "        df_datas.append(df_data_single)\n",
    "    \n",
    "    #len(set.union(*[set(x.columns) for x in df_datas]))  120 columns now, but for one region there are only 80, intesection 44\n",
    "    #len(set.intersection(*[set(x.columns) for x in df_datas]))  # 120 columns now, but for one region there are only 80\n",
    "    \n",
    "    return df_datas, region_names\n",
    "        \n",
    "\n",
    "def _is_column_in_row_inalid(rc):\n",
    "    # all_zeros = not np.any(rc) - old version - all values 0\n",
    "    # new version - 2 interesting values 0 (it happens sometimes that only one channel is not zero)\n",
    "    return rc[4-1] == 0 and rc[8-1] == 0  \n",
    "    \n",
    "    \n",
    "def _find_closest_non_zero_column(time_index, common_days, dates_list, row):\n",
    "    # this time step is zero, we needto find another one that is not zero.\n",
    "    # To do it, find all closest non-zero columns (for all time steps for this row) \n",
    "        \n",
    "    time_distance_to_nonzero_columns = [abs(common_days[time_index] - v) for v in dates_list]\n",
    "    for k in range(len(time_distance_to_nonzero_columns)):\n",
    "        #if not np.any(row.iloc[k]):\n",
    "        if _is_column_in_row_inalid(row.iloc[k]):\n",
    "            time_distance_to_nonzero_columns[k] = 9999\n",
    "    closest_nonzero_column = np.argmin(time_distance_to_nonzero_columns)\n",
    "    return closest_nonzero_column\n",
    "\n",
    "    \n",
    "def _resample_and_concatenate_regions_data(df_datas, resampled_days_interval):\n",
    "    # Conatenation of data with different dates - fixed interval span, with finding closes date (better to use interpolation, but not with nois cloud data)\n",
    "    DI = resampled_days_interval  # days interval\n",
    "    common_days = list(range(DI, 365, DI))\n",
    "    print(f'len(common_days) = {len(common_days)}')\n",
    "    # common_days_datetime = [for day in common_days]\n",
    "\n",
    "    # year = int(timesteps[10][:4])\n",
    "    # new_year_day = dt.datetime(year=year, month=1, day=1)\n",
    "    # dates_list = [((dt.datetime.strptime(date, tf)- new_year_day).days + 1) for date in timesteps]\n",
    "\n",
    "    df_data_all = pd.DataFrame(columns=common_days)\n",
    "\n",
    "\n",
    "    for df_data_single in tqdm(df_datas):\n",
    "        timesteps = list(df_data_single.columns)\n",
    "        year = int(timesteps[10][:4])\n",
    "        new_year_day = dt.datetime(year=year, month=1, day=1)\n",
    "        tf = '%Y%m%d'\n",
    "        dates_list = [((dt.datetime.strptime(date, tf)- new_year_day).days + 1) for date in timesteps]\n",
    "        df_data_single = df_data_single.rename(columns={old: new for old, new in zip(timesteps, dates_list)})\n",
    "\n",
    "        closest_columns = []\n",
    "        for common_day in common_days:\n",
    "            closest_column = np.argmin([abs(common_day - v) for v in dates_list])\n",
    "            closest_columns.append(closest_column)\n",
    "\n",
    "        new_frames = []\n",
    "        for index, row in df_data_single.iterrows():\n",
    "            resampled_row_data = []\n",
    "            \n",
    "            for i, closest_column in enumerate(closest_columns):\n",
    "                rc = row.iloc[closest_column]\n",
    "                invalid_rc = _is_column_in_row_inalid(rc)\n",
    "                if invalid_rc:\n",
    "                    closest_nonzero_column = _find_closest_non_zero_column(\n",
    "                        time_index=i,\n",
    "                        common_days=common_days,\n",
    "                        dates_list=dates_list, \n",
    "                        row=row)\n",
    "                    rc = row.iloc[closest_nonzero_column]\n",
    "                \n",
    "                resampled_row_data.append(rc)\n",
    "\n",
    "            resampled_row_df = pd.DataFrame([resampled_row_data], columns=common_days, index=[index])\n",
    "            new_frames.append(resampled_row_df)\n",
    "\n",
    "        new_frames_df = pd.concat(new_frames)\n",
    "        df_data_all = pd.concat([df_data_all, new_frames_df])\n",
    "    \n",
    "    return df_data_all, common_days\n",
    "\n",
    "\n",
    "def _load_all_labels(region_names):\n",
    "    df_labels_all_lists = []\n",
    "    for region_name in region_names:\n",
    "        region_name = region_name.strip('/')\n",
    "        LABELS_CSV_FILE_PATH = os.path.join(ROOT_DATA_PATH, f'csv_labels/train/demo_eurocrops_{region_name}.csv')\n",
    "        GEO_JSON_FILE_PATH = os.path.join(ROOT_DATA_PATH, f'GeoJSONs_regional_split/train/AT/demo_eurocrops_{region_name}.geojson')\n",
    "\n",
    "        # csv_file_path = os.path.join(train_csv_dir, csv_file_name)\n",
    "        df_labels = pd.read_csv(LABELS_CSV_FILE_PATH, index_col=0)\n",
    "        df_labels_all_lists.append(df_labels)\n",
    "\n",
    "\n",
    "    df_labels_all = pd.concat(df_labels_all_lists)\n",
    "    return df_labels_all\n",
    "    \n",
    "\n",
    "@memory.cache\n",
    "def load_all_data_from_file_resampled(\n",
    "        h5_file_path: str, \n",
    "        resampled_days_interval: int,\n",
    "        ):\n",
    "    df_datas, region_names = _load_data_from_h5_file(h5_file_path=h5_file_path)\n",
    "    df_data_all, common_days = _resample_and_concatenate_regions_data(df_datas=df_datas, resampled_days_interval=resampled_days_interval)\n",
    "    \n",
    "    df_labels_all = _load_all_labels(region_names=region_names)\n",
    "\n",
    "    return df_data_all, df_labels_all, common_days, region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_all, df_labels_all, common_days, region_names = load_all_data_from_file_resampled(\n",
    "    h5_file_path=H5_FILE_PATH, \n",
    "    resampled_days_interval=7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_all.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out the data for one parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pick the first row\n",
    "# example_row = df_data_all.iloc[0]\n",
    "# parcel_ID = example_row.name\n",
    "\n",
    "# # Get the corresponding label\n",
    "# label_code = df_labels_all.loc[parcel_ID]['crpgrpc']\n",
    "# label_name = df_labels_all.loc[parcel_ID]['crpgrpn']\n",
    "\n",
    "# print('{} grows on parcel {}'.format(label_name, parcel_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_row_np = example_row.to_numpy()\n",
    "# example_row_np = np.stack(example_row_np, axis=0)\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [15, 8]\n",
    "# plt.plot(common_days, example_row_np)\n",
    "# # plt.legend(bands)\n",
    "# plt.style.use('_classic_test_patch')\n",
    "# plt.xlabel('day of year')\n",
    "# plt.ylabel('channel value')\n",
    "# plt.title(f'Data for parcel id {parcel_ID}')\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geometry_dict_by_parcelid_all(region_names):\n",
    "    geometry_dict_by_parcelid_all = {}\n",
    "    for region_name in tqdm(region_names):\n",
    "        region_name = region_name.strip('/')\n",
    "        GEO_JSON_FILE_PATH = os.path.join(ROOT_DATA_PATH, f'GeoJSONs_regional_split/train/AT/demo_eurocrops_{region_name}.geojson')    \n",
    "\n",
    "        with open(GEO_JSON_FILE_PATH, 'r') as file:\n",
    "            geojson_data = json.load(file)\n",
    "\n",
    "        geometry_dict_by_parcelid = {feature['properties']['recno']: feature['geometry'] \n",
    "                                     for feature in geojson_data['features']}\n",
    "        geometry_dict_by_parcelid_all.update(geometry_dict_by_parcelid)\n",
    "    return geometry_dict_by_parcelid_all\n",
    "\n",
    "\n",
    "geometry_dict_by_parcelid_all = load_geometry_dict_by_parcelid_all(region_names=region_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check crop types in the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_types_counts_and_ids(df_data_all, df_labels_all):\n",
    "    crop_types_counts = {}\n",
    "    crop_types_ids = {}\n",
    "\n",
    "    regions_id_set = set(df_data_all.index)\n",
    "    \n",
    "    for i, region_id in enumerate(df_labels_all.index):\n",
    "        if region_id not in regions_id_set:\n",
    "            continue\n",
    "\n",
    "        crop_name = df_labels_all.iloc[i]['crpgrpn']\n",
    "        current_count = crop_types_counts.get(crop_name, 0) \n",
    "        crop_types_counts[crop_name] = current_count + 1\n",
    "\n",
    "        if crop_name not in crop_types_ids:\n",
    "            crop_types_ids[crop_name] = []\n",
    "        crop_types_ids[crop_name].append(region_id)\n",
    "    \n",
    "    return crop_types_counts, crop_types_ids\n",
    "\n",
    "\n",
    "        \n",
    "crop_types_counts, crop_types_ids = get_crop_types_counts_and_ids(df_data_all=df_data_all, df_labels_all=df_labels_all)\n",
    "\n",
    "print(f'Total crop fields: {sum(crop_types_counts.values())}')\n",
    "crop_types_counts = {k: v for k, v in sorted(crop_types_counts.items(), key=lambda item: -item[1])}\n",
    "crop_types_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_labels_all), len(df_data_all), len(geometry_dict_by_parcelid_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_crop_type(crop_name):\n",
    "    data = np.zeros(shape=(len(crop_types_ids[crop_name]), len(common_days), NUMBER_OF_CHANNELS), dtype=float)\n",
    "    for i, region_id in enumerate(crop_types_ids[crop_name]):\n",
    "        region_data = df_data_all.loc[region_id].to_numpy()\n",
    "        data[i, ...] = np.stack(region_data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class CropNdviData:\n",
    "#     mean: np.ndarray\n",
    "#     std: np.ndarray\n",
    "\n",
    "        \n",
    "def calc_ndvi(B4, B8):\n",
    "    return (B8 - B4) / (B8 + B4)\n",
    "        \n",
    "    \n",
    "# def get_ndvi_data(data_crop) -> CropNdviData:\n",
    "#     \"\"\"\n",
    "#     data_crop: [fields (for the crop), time (common_days), channels (bands B0-B12)]')\n",
    "#     return: mean and std for ndvi \"channel\"\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # B8-B4 / (B8+B4)   ( counting from B1 to B13)\n",
    "#     B4 = data_crop[:, :, 4-1]\n",
    "#     B8 = data_crop[:, :, 8-1]\n",
    "\n",
    "#     data_crop_ndvi = calc_ndvi(B4, B8)\n",
    "#     data_crop_mean_ndvi = np.mean(data_crop_ndvi, axis=0)\n",
    "#     data_crop_std_ndvi = np.std(data_crop_ndvi, axis=0) \n",
    "    \n",
    "#     return CropNdviData(mean=data_crop_mean_ndvi, std=data_crop_std_ndvi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flatten_list = []  \n",
    "data_flatten_ndvi_list = []\n",
    "\n",
    "\n",
    "for index, row in df_data_all.iterrows():\n",
    "    region_data = row.to_numpy()\n",
    "    region_data_stacked = np.stack(region_data)\n",
    "    region_data_stacked_flat = region_data_stacked.flatten('F')  # so the first channel through time is continous\n",
    "    data_flatten_list.append(region_data_stacked_flat)\n",
    "\n",
    "    B4 = region_data_stacked[:, 4-1]\n",
    "    B8 = region_data_stacked[:, 8-1]\n",
    "    region_data_ndvi = calc_ndvi(B4, B8)\n",
    "    data_flatten_ndvi_list.append(region_data_ndvi)\n",
    "    \n",
    "data_flatten = np.stack(data_flatten_list)\n",
    "data_ndvi_flatten = np.stack(data_flatten_ndvi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flatten.shape, data_ndvi_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crop_types = set(df_labels_all.iloc[:, 1].to_numpy())\n",
    "data_flatten_labels = df_labels_all.iloc[:, 1].to_numpy()\n",
    "print(set(data_flatten_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data for processing\n",
    "\n",
    "#### choose crop types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# crops_to_use__names = [\n",
    "#     #'pasture_meadow',\n",
    "#     'grain_maize',\n",
    "#     #'winter_common_wheat_and_spelt',\n",
    "#     #'other_plants_harvested_green',\n",
    "#     'winter_barley',\n",
    "#     'vineyards',\n",
    "#     'soya',\n",
    "#     'sugar_beet',\n",
    "#     'winter_triticale',\n",
    "#     'winter_rye',\n",
    "#     'leguminous_plants',\n",
    "#     'sunflower_and_yellow_bloomer',\n",
    "#     #'other_cereals_for_the_production_of_grain',\n",
    "#     'millet',\n",
    "#     'winter_rape',\n",
    "#     #'fresh_vegetables_melons_and_strawberries',\n",
    "#     'summer_barley',\n",
    "#     #'fruit_of_temperate_climate_zones',\n",
    "#     'potatoes',\n",
    "#     'summer_oats',\n",
    "#     'cucurbits',\n",
    "#     #'other_dry_pulses',\n",
    "#     'summer_durum_wheat',\n",
    "#     #'arable_land_seed_and_seedlings',\n",
    "#     'winter_durum_wheat',\n",
    "#     #'aromatic_plants_medicinal_and_culinary_plants',\n",
    "#     #'energy_crops',\n",
    "#     #'summer_common_wheat_and_spelt',\n",
    "#     #'temporary_grass',\n",
    "#     #'other_oil_seed_crops',\n",
    "#     'hemp',\n",
    "#     'nuts',\n",
    "#     #'flowers_and_ornamental_plants',\n",
    "#     #'berry_species',\n",
    "#     #'nurseries',\n",
    "# ]\n",
    "crops_to_use__names = [ \n",
    "#     'sunflower_and_yellow_bloomer',\n",
    "#     #'nuts',\n",
    "#     'soya',\n",
    "#     'vineyards',\n",
    "#     'potatoes',\n",
    "#     'summer_barley',\n",
    "    \n",
    "#     'cucurbits',\n",
    "#     'sugar_beet',\n",
    "#     'millet',\n",
    "#     'grain_maize',\n",
    "    \n",
    "    'vineyards',\n",
    "    'sunflower_and_yellow_bloomer',\n",
    "    #'nuts',\n",
    "    'sugar_beet',\n",
    "    'nuts',\n",
    "    'hemp',\n",
    "    'summer_durum_wheat',\n",
    "    'hops',\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "assert(len(crops_to_use__names) == len(set(crops_to_use__names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_use = []\n",
    "for i in range(len(df_labels_all)):\n",
    "    label = data_flatten_labels[i]\n",
    "    if label not in crops_to_use__names:\n",
    "        continue  # skip this data, not interested\n",
    "    indexes_to_use.append(i)\n",
    "# indexes_to_use = np.array(indexes_to_use)\n",
    "\n",
    "selected_data_flatten_labels = np.take(data_flatten_labels, indexes_to_use)\n",
    "selected_data_flatten = data_flatten[indexes_to_use, :]\n",
    "selected_data_ndvi_flatten = data_ndvi_flatten[indexes_to_use, :]\n",
    "len(set(selected_data_flatten_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_flatten_labels.shape, selected_data_flatten.shape, selected_data_ndvi_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_flatten_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ndvi_flatten[[1,2,3,], :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remap output classes to consecutive values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_data_flatten_labels_mapping = {value: i for i, value in enumerate(crops_to_use__names)}\n",
    "\n",
    "selected_data_flatten_labels_mapped = np.full(shape=selected_data_flatten_labels.shape, fill_value=1, dtype=int)\n",
    "for i in range(len(selected_data_flatten_labels_mapped)):\n",
    "    selected_data_flatten_labels_mapped[i] = selected_data_flatten_labels_mapping[selected_data_flatten_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_flatten_labels_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(selected_data_flatten_labels_mapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the data type to use (for further processing)\n",
    "And split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = selected_data_ndvi_flatten\n",
    "\n",
    "y = selected_data_flatten_labels_mapped\n",
    "# print(len(x))\n",
    "NUMBER_OF_CLASSES = len(set(y))\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "n1, n2 = int(len(y) * 0.8), int(len(y) * 0.9)\n",
    "x_shuffled, y_shuffled = unison_shuffled_copies(x, y)\n",
    "x_train, x_valid, x_test = x_shuffled[:n1], x_shuffled[n1:n2], x_shuffled[n2:]\n",
    "y_train, y_valid, y_test = y_shuffled[:n1], y_shuffled[n1:n2], y_shuffled[n2:]\n",
    "\n",
    "\n",
    "len(x_train), len(x_valid), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes distribution and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(y_test, return_counts=True)\n",
    "random_guess_accuraccy = max(counts) / sum(counts)\n",
    "print(f'{random_guess_accuraccy = :.3f}')\n",
    "\n",
    "\n",
    "counts_by_class = Counter(y_train)\n",
    "# weights_for_classes = np.array([counts_by_class[i] / len(y_train) for i in range(NUMBER_OF_CLASSES)])  # the weights must sum up to 1?\n",
    "weights_for_classes = np.array([1 / (counts_by_class[i]) for i in range(NUMBER_OF_CLASSES)])  # the weights must sum up to 1?\n",
    "weights_for_classes = weights_for_classes / (weights_for_classes.sum())\n",
    "\n",
    "weights_for_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CLASS_WEIGHTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_TSNE_NDVI = False\n",
    "\n",
    "\n",
    "if PLOT_TSNE_NDVI:\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "    z = tsne.fit_transform(x)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"y\"] = selected_data_flatten_labels\n",
    "    df[\"comp-1\"] = z[:, 0]\n",
    "    df[\"comp-2\"] = z[:, 1]\n",
    "\n",
    "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                    palette=sns.color_palette(\"hls\", len(set(selected_data_flatten_labels))),\n",
    "                    data=df).set(title=\"selected_data_flatten_labels crops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_TSNE_NDVI:\n",
    "    import seaborn as sns\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    tsne = TSNE(n_components=3, verbose=1, random_state=123)\n",
    "    z = tsne.fit_transform(x)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(z[:, 0], z[:, 1], z[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_TSNE_NDVI:\n",
    "    fig = px.scatter_3d(\n",
    "        z, x=0, y=1, z=2,\n",
    "        color=y, labels={'color': 'species'}\n",
    "    )\n",
    "    fig.update_traces(marker_size=8)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(class_weight='balanced' if USE_CLASS_WEIGHTS else None)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_predicted = clf.predict(x_test)\n",
    "\n",
    "predicted_correctly = np.count_nonzero(y_predicted == y_test)\n",
    "predicted_correctly, len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_matrix_for_test_data(y_predicted_labels, y_test_labels, normalize='true'):\n",
    "    svm_ndvi_confusion_matrix = sklearn.metrics.confusion_matrix(y_true=y_test_labels, y_pred=y_predicted_labels, \n",
    "                                                                 normalize=normalize)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(svm_ndvi_confusion_matrix, display_labels=crops_to_use__names)\n",
    "    disp.plot()\n",
    "    \n",
    "    \n",
    "    predicted_correctly = np.count_nonzero(y_predicted_labels == y_test_labels)\n",
    "\n",
    "    accuraccy = predicted_correctly / len(y_test_labels)\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Cohen%27s_kappa\n",
    "    kappa = 1 - (1 - accuraccy) / (1 - random_guess_accuraccy)\n",
    "\n",
    "    print(f'svm_ndvi_kappa={kappa:.3f}, accuraccy={accuraccy:.3f}')\n",
    "    \n",
    "\n",
    "draw_prediction_matrix_for_test_data(y_predicted, y_test)\n",
    "draw_prediction_matrix_for_test_data(y_predicted, y_test, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_INPUTS = x[0].shape[0]\n",
    "print(f'NUMBER_OF_INPUTS = {NUMBER_OF_INPUTS}, NUMBER_OF_CLASSES={NUMBER_OF_CLASSES}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=NUMBER_OF_INPUTS, out_features=200),\n",
    "    nn.BatchNorm1d(num_features=200),  # batchnorm before activation\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(in_features=200, out_features=100),\n",
    "    nn.BatchNorm1d(num_features=100),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(in_features=100, out_features=50),\n",
    "    nn.BatchNorm1d(num_features=50),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # nn.Dropout(0.05),\n",
    "    nn.Linear(in_features=50, out_features=NUMBER_OF_CLASSES),\n",
    "    nn.Softmax(dim=-1),  # not 0!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     o = model(torch.Tensor(x_train[0:2]))\n",
    "# o\n",
    "# np.sum(o.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert len(x) == len(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "        return self.x[idx].astype(np.float32), self.y[idx]\n",
    "\n",
    "\n",
    "batch_size = 512  # 64\n",
    "trainloader = torch.utils.data.DataLoader(CropDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(CropDataset(x_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(CropDataset(x_test, y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "next(iter(trainloader))[0].shape\n",
    "trainloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_and_true_labels(loader):\n",
    "    y_predicted_labels = []\n",
    "    y_test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.train(False)\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            label_predicted = np.argmax(outputs.numpy(), axis=1)\n",
    "            y_predicted_labels.append(label_predicted)\n",
    "            y_test_labels.append(labels)\n",
    "\n",
    "\n",
    "    y_test_labels = np.concatenate(y_test_labels)\n",
    "    y_predicted_labels = np.concatenate(y_predicted_labels)\n",
    "\n",
    "    predicted_correctly = np.count_nonzero(y_predicted_labels == y_test_labels)\n",
    "    accuraccy = predicted_correctly / len(y_test_labels)\n",
    "    print(f'{accuraccy = :.3f}')\n",
    "    return y_predicted_labels, y_test_labels, accuraccy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "       layer.reset_parameters()\n",
    "       \n",
    "train_loss_vec = []\n",
    "valid_loss_vec = []\n",
    "accuraccy_vec = []\n",
    "lr_vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(weights_for_classes.astype(np.float32)))\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)  # with batch_size 512\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "for epoch in range(60):  # 40\n",
    "    for phase in ['train', 'valid']:\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        if phase == 'train':\n",
    "            model.train(True)  # Set trainind mode = true\n",
    "            dataloader = trainloader\n",
    "        else:\n",
    "            model.train(False)  # Set model to evaluate mode\n",
    "            dataloader = validloader\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()  # zero the parameter gradients\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_loss = running_loss / len(dataloader.dataset)\n",
    "            train_loss_vec.append(train_loss)\n",
    "        else:\n",
    "            valid_loss = running_loss / len(dataloader.dataset)\n",
    "            valid_loss_vec.append(valid_loss)\n",
    "\n",
    "   \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(valid_loss)\n",
    "    print(f'[{epoch + 1}]\\t train loss: {train_loss:.6f}\\t valid loss: {valid_loss:.6f}, {current_lr = }')\n",
    "    _, _, accuraccy = get_predicted_and_true_labels(validloader)\n",
    "    accuraccy_vec.append(accuraccy)\n",
    "    lr_vec.append(current_lr)\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_vec)\n",
    "plt.plot(valid_loss_vec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuraccy_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels_test, y_test_labels, _ = get_predicted_and_true_labels(testloader)\n",
    "draw_prediction_matrix_for_test_data(y_predicted_labels_test, y_test_labels)\n",
    "draw_prediction_matrix_for_test_data(y_predicted_labels_test, y_test_labels, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "453c204b3f8c7b1daec18e369dbca41a43ef83b53879b5da501a9e5377e85acd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
